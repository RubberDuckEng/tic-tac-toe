{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from random import shuffle\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1655964169813
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH = 3\n",
        "HEIGHT = 3\n",
        "ACTION_COUNT = 9\n",
        "\n",
        "PLAYERS = {\n",
        "    -1: \"O\",\n",
        "    1: \"X\",\n",
        "}\n",
        "\n",
        "def get_action_coords(action_index):\n",
        "    return action_index % 3, action_index // 3\n",
        "\n",
        "class Board(object):\n",
        "    def __init__(self, pieces):\n",
        "        assert(pieces.size() == (HEIGHT, WIDTH))\n",
        "        self.pieces = pieces\n",
        "\n",
        "    @classmethod\n",
        "    def initial(cls):\n",
        "        return cls(torch.zeros(3, 3))\n",
        "\n",
        "    def is_valid_action(self, action_index):\n",
        "        x, y = get_action_coords(action_index)\n",
        "        return self.pieces[y][x] == 0\n",
        "    \n",
        "    def get_valid_actions(self):\n",
        "        return [index for index in range(ACTION_COUNT) if self.is_valid_action(index)]\n",
        "\n",
        "    def get_valid_action_mask(self):\n",
        "        return torch.tensor([1 if self.is_valid_action(index) else 0 for index in range(ACTION_COUNT)])\n",
        "\n",
        "    def has_empty_slots(self):\n",
        "        return len(self.get_valid_actions()) > 0\n",
        "\n",
        "    def execute_action(self, player, action_index):\n",
        "        assert(player in PLAYERS.keys())\n",
        "        assert(self.is_valid_action(action_index))\n",
        "        x, y = get_action_coords(action_index)\n",
        "        self.pieces[y][x] = player\n",
        "\n",
        "    def has_won(self, player):\n",
        "        assert(player in PLAYERS.keys())\n",
        "        target = 3 * player\n",
        "        for x in range(3):\n",
        "            if torch.sum(self.pieces[:, x]) == target:\n",
        "                return True\n",
        "        for y in range(3):\n",
        "            if torch.sum(self.pieces[y, :]) == target:\n",
        "                return True\n",
        "        if sum([self.pieces[i, i].item() for i in range(3)]) == target:\n",
        "            return True\n",
        "        if sum([self.pieces[2 - i, i].item() for i in range(3)]) == target:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def get_score(self):\n",
        "        for player in PLAYERS.keys():\n",
        "            if self.has_won(player):\n",
        "                return player\n",
        "        if not self.has_empty_slots():\n",
        "            return 0\n",
        "        return None\n",
        "\n",
        "    def get_random_action(self):\n",
        "        legal_actions = self.get_valid_actions()\n",
        "        legal_ACTION_COUNT = len(legal_actions)\n",
        "        if legal_ACTION_COUNT == 0:\n",
        "            return None\n",
        "        return legal_actions[random.randrange(legal_ACTION_COUNT)]\n",
        "\n",
        "\n",
        "def get_next_state(pieces, player, action):\n",
        "    board = Board(torch.clone(pieces))\n",
        "    board.execute_action(player, action)\n",
        "    return board.pieces, -player\n",
        "\n",
        "\n",
        "def get_canonical_form(pieces, player):\n",
        "    return player * pieces\n",
        "\n",
        "\n",
        "_SLOTS = {\n",
        "    -1: \"O\",\n",
        "    0: \"-\",\n",
        "    1: \"X\",\n",
        "}\n",
        "\n",
        "def get_repr(pieces):\n",
        "    return \"\".join([_SLOTS[slot.item()] for slot in pieces.view(9)])\n",
        "\n",
        "# TODO: Symmetries\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655964083550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIMULATION_COUNT = 25\n",
        "C_PUCT = 1.0\n",
        "EPS = 1e-8\n",
        "\n",
        "class MonteCarloTreeSearch(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
        "        self.Nsa = {}  # stores #times edge s,a was visited\n",
        "        self.Ns = {}  # stores #times board s was visited\n",
        "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
        "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
        "        self.Vs = {}  # stores game.getValidactions for board s\n",
        "\n",
        "    def get_actions(self, pieces, temp=1):\n",
        "        for i in range(SIMULATION_COUNT):\n",
        "            self.search(pieces)\n",
        "\n",
        "        s = get_repr(pieces)\n",
        "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(ACTION_COUNT)]\n",
        "        counts = [n ** (1. / temp) for n in counts]\n",
        "        total = float(sum(counts))\n",
        "        return [n / total for n in counts]\n",
        "\n",
        "    def search(self, pieces):\n",
        "        s = get_repr(pieces)\n",
        "        board = Board(pieces)\n",
        "\n",
        "        if s not in self.Es:\n",
        "            self.Es[s] = board.get_score()\n",
        "        if self.Es[s] is not None:\n",
        "            # terminal node\n",
        "            return -self.Es[s]\n",
        "\n",
        "        if s not in self.Ps:\n",
        "            # leaf node\n",
        "            self.Ps[s], v = self.model.predict(pieces)\n",
        "            valids = board.get_valid_action_mask()\n",
        "            self.Ps[s] = self.Ps[s] * valids  # masking invalid actions\n",
        "            sum_Ps_s = torch.sum(self.Ps[s])\n",
        "            if sum_Ps_s > 0:\n",
        "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
        "            else:\n",
        "                self.Ps[s] = self.Ps[s] + valids\n",
        "                self.Ps[s] /= torch.sum(self.Ps[s])\n",
        "\n",
        "            self.Vs[s] = valids\n",
        "            self.Ns[s] = 0\n",
        "            return -v\n",
        "\n",
        "        valids = self.Vs[s]\n",
        "        best_upper_confidence = -float('inf')\n",
        "        best_action = -1\n",
        "\n",
        "        for a in range(ACTION_COUNT):\n",
        "            if valids[a]:\n",
        "                if (s, a) in self.Qsa:\n",
        "                    u = self.Qsa[(s, a)] + C_PUCT * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (1 + self.Nsa[(s, a)])\n",
        "                else:\n",
        "                    u = C_PUCT * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
        "\n",
        "                if u > best_upper_confidence:\n",
        "                    best_upper_confidence = u\n",
        "                    best_action = a\n",
        "\n",
        "        a = best_action\n",
        "        next_pieces, next_player = get_next_state(pieces, 1, a)\n",
        "        next_pieces = get_canonical_form(next_pieces, next_player)\n",
        "\n",
        "        v = self.search(next_pieces)\n",
        "\n",
        "        if (s, a) in self.Qsa:\n",
        "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
        "            self.Nsa[(s, a)] += 1\n",
        "        else:\n",
        "            self.Qsa[(s, a)] = v\n",
        "            self.Nsa[(s, a)] = 1\n",
        "\n",
        "        self.Ns[s] += 1\n",
        "        return -v\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655961973584
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INTERATION_COUNT = 10 #00\n",
        "EPISODE_COUNT = 10 #0\n",
        "\n",
        "Observation = namedtuple('Observation', ('board', 'player', 'pi'))\n",
        "\n",
        "class Learner(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.mcts = None\n",
        "        self.history = deque([], maxlen=20)\n",
        "\n",
        "    def play_game(self):\n",
        "        pieces = Board.initial().pieces\n",
        "        player = 1\n",
        "        observations = []\n",
        "        while True:\n",
        "            canonical_pieces = get_canonical_form(pieces, player)\n",
        "            # TODO: Adjust temperature\n",
        "            pi = self.mcts.get_actions(canonical_pieces)\n",
        "            # TODO: Regularize using symmetries\n",
        "            observations.append(Observation(pieces, player, pi))\n",
        "            action = np.random.choice(len(pi), pi)\n",
        "            pieces, player = get_next_state(pieces, player, action)\n",
        "            score = Board(pieces).get_score()\n",
        "            if score is not None:\n",
        "                return [(obs.board, obs.pi, score * (-1) ** (obs.player != player)) for obs in observations]\n",
        "\n",
        "    def learn(self):\n",
        "        for i in range(INTERATION_COUNT):\n",
        "            print(\"Starting iteration f{i} ...\")\n",
        "            observations = deque([], maxlen=20000)\n",
        "            for _ in tqdm(range(EPISODE_COUNT), desc=\"Self Play\"):\n",
        "                self.mcts = MonteCarloTreeSearch(self.model)\n",
        "                observations += self.play_game()\n",
        "            self.history.append(observations)\n",
        "\n",
        "            dataset = []\n",
        "            for observations in self.history:\n",
        "                dataset.extend(observations)\n",
        "            shuffle(dataset)\n",
        "            self.model.train(dataset)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655963982260
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(9, 32)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.fc3 = nn.Linear(32, ACTION_COUNT)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size()[0], -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        pi = self.fc3(out)\n",
        "        v = self.fc4(out)\n",
        "        return F.log_softmax(pi, dim=1), torch.tanh(v)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655964302158
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, nnet):\n",
        "        self.nnet = nnet\n",
        "\n",
        "    def train(self, dataset):\n",
        "        pass\n",
        "    \n",
        "    def predict(self, pieces):\n",
        "        self.nnet.eval()\n",
        "        pieces = pieces.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            pi, v = self.nnet(pieces)\n",
        "        return torch.exp(pi)[0], v[0].item()"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655964715206
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nnet = Network()\n",
        "model = Model(nnet)\n",
        "board = Board.initial()\n",
        "model.predict(board.pieces)\n",
        "\n",
        "# learner = Learner(model)\n",
        "# learner.learn()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "(tensor([0.0969, 0.0979, 0.1316, 0.1287, 0.0978, 0.1055, 0.1163, 0.1298, 0.0956]),\n 0.010271133854985237)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655964716633
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestBoard(unittest.TestCase):\n",
        "    def test_legal_actions(self):\n",
        "        board = Board.initial()\n",
        "        self.assertTrue(board.has_empty_slots())\n",
        "        self.assertListEqual(list(range(ACTION_COUNT)), board.get_valid_actions())\n",
        "        self.assertFalse(board.has_won(1))\n",
        "        self.assertFalse(board.has_won(-1))\n",
        "\n",
        "    def test_execute_action(self):\n",
        "        board = Board.initial()\n",
        "        self.assertTrue(board.is_valid_action(1))\n",
        "        board.execute_action(1, 1)\n",
        "        self.assertFalse(board.is_valid_action(1))\n",
        "        self.assertEqual(ACTION_COUNT - 1, len(board.get_valid_actions()))\n",
        "\n",
        "    def test_has_won(self):\n",
        "        board = Board.initial()\n",
        "        board.execute_action(1, 0)\n",
        "        board.execute_action(1, 1)\n",
        "        self.assertFalse(board.has_won(1))\n",
        "        self.assertFalse(board.has_won(-1))\n",
        "        self.assertEqual(None, board.get_score())\n",
        "        board.execute_action(1, 2)\n",
        "        self.assertTrue(board.has_won(1))\n",
        "        self.assertFalse(board.has_won(-1))\n",
        "        self.assertEqual(1, board.get_score())\n",
        "\n",
        "    def test_has_won_diagonal(self):\n",
        "        board = Board.initial()\n",
        "        board.execute_action(-1, 0)\n",
        "        board.execute_action(-1, 4)\n",
        "        self.assertFalse(board.has_won(1))\n",
        "        self.assertFalse(board.has_won(-1))\n",
        "        self.assertEqual(None, board.get_score())\n",
        "        board.execute_action(-1, 8)\n",
        "        self.assertFalse(board.has_won(1))\n",
        "        self.assertTrue(board.has_won(-1))\n",
        "        self.assertEqual(-1, board.get_score())\n",
        "\n",
        "\n",
        "class RandomModel(object):\n",
        "    def predict(self, pieces):\n",
        "        return torch.rand(ACTION_COUNT), random.randrange(-1, 1)\n",
        "\n",
        "\n",
        "class TestMCTS(unittest.TestCase):\n",
        "    def test_get_actions(self):\n",
        "        model = RandomModel()\n",
        "        mcts = MonteCarloTreeSearch(model)\n",
        "        board = Board.initial()\n",
        "        actions = mcts.get_actions(board.pieces)\n",
        "        self.assertEqual(ACTION_COUNT, len(actions))\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "test_execute_move (__main__.TestBoard) ... ok\ntest_has_won (__main__.TestBoard) ... ok\ntest_has_won_diagonal (__main__.TestBoard) ... ok\ntest_legal_moves (__main__.TestBoard) ... ok\ntest_get_actions (__main__.TestMCTS) ... ok\n\n----------------------------------------------------------------------\nRan 5 tests in 0.017s\n\nOK\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "<unittest.main.TestProgram at 0x7f5b1d0f9c40>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655961872570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}