{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda44117-b4d6-494b-958d-a2f17a9cafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50827e9-539f-4963-a32e-d7276db4d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions = 9\n",
    "\n",
    "def create_board():\n",
    "    us = torch.zeros(3, 3)\n",
    "    them = torch.zeros(3, 3)\n",
    "    return torch.stack([us, them])\n",
    "\n",
    "def get_move_coords(move_index):\n",
    "    return move_index % 3, move_index // 3\n",
    "\n",
    "def is_valid_move(state, move_index):\n",
    "    if move_index < 0 or move_index >= 9:\n",
    "        return False\n",
    "    x, y = get_move_coords(move_index)\n",
    "    return torch.all(state[:, y, x] == 0).item()\n",
    "\n",
    "def get_invalid_move_mask(state):\n",
    "    return torch.tensor([not is_valid_move(state, move_index)\n",
    "                         for move_index in range(number_of_actions)])\n",
    "\n",
    "def make_move(state, move_index):\n",
    "    assert(is_valid_move(state, move_index))\n",
    "    x, y = get_move_coords(move_index)\n",
    "    us, them = state.unbind()\n",
    "    us = us.clone()\n",
    "    us[y, x] = 1\n",
    "    return torch.stack([them, us])\n",
    "\n",
    "def has_won(ply):\n",
    "    for x in range(3):\n",
    "        if torch.sum(ply[:, x]) == 3:\n",
    "            return True\n",
    "    for y in range(3):\n",
    "        if torch.sum(ply[y, :]) == 3:\n",
    "            return True\n",
    "    if sum([ply[i, i].item() for i in range(3)]) == 3:\n",
    "        return True\n",
    "    if sum([ply[2 - i, i].item() for i in range(3)]) == 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_score(state):\n",
    "    us, them = state.unbind()\n",
    "    if has_won(us):\n",
    "        return 1 \n",
    "    if has_won(them):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def is_complete(state):\n",
    "    if torch.sum(state, (-3, -2, -1)) == 9:\n",
    "        return True\n",
    "    return get_score(state) != 0\n",
    "\n",
    "def get_random_move(state):\n",
    "    if is_complete(state):\n",
    "        return None\n",
    "    while True:\n",
    "        candidate_move_index = random.randrange(number_of_actions)\n",
    "        if is_valid_move(state, candidate_move_index):\n",
    "            return candidate_move_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a848bc-b348-4d28-863f-32605d2e6818",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Q equation\n",
    "\n",
    "`Q(state, a) => reward`\n",
    "\n",
    "`GAMMA` is a discount factor for future rewards.\n",
    "\n",
    "If `state'` is non-final:\n",
    "\n",
    "`Q(state, a) = reward + GAMMA * max_{a'} Q(state', a')`\n",
    "\n",
    "Otherwise, `state'` is final, which means no more actions are possible:\n",
    "\n",
    "`Q(state, a) = reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17580f8c-091d-4484-a7bc-a4c021542885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.number_of_actions = 2\n",
    "# self.gamma = 0.99\n",
    "# self.final_epsilon = 0.0001\n",
    "# self.initial_epsilon = 0.1\n",
    "# self.number_of_iterations = 2000000\n",
    "# self.replay_memory_size = 10000\n",
    "# self.minibatch_size = 32\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(18, 32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(32, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size()[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1389da-2c6d-414c-b07c-861e44c2f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ba035a-b9c6-4daf-8e76-be4b5ebedb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047a9c75-8201-4080-bdd4-030f5e508e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e08786-6b76-4983-b465-ddd8902f4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435d3ea0-7550-487a-b135-da4261ddbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Prevent the selection of an invalid move by replacing\n",
    "            # all the invalid moves with -inf\n",
    "            actions = model(torch.stack([state]))\n",
    "            mask = get_invalid_move_mask(state)\n",
    "            actions.masked_fill_(mask, float('-inf'))\n",
    "            return actions.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[get_random_move(state)]], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a05c5d-d064-4463-ab7b-7f6d7925debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to the model\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # the model; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e93680-2758-4d92-8464-eb4ff752beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 of 10000\n",
      "episode 500 of 10000\n",
      "episode 1000 of 10000\n",
      "episode 1500 of 10000\n",
      "episode 2000 of 10000\n",
      "episode 2500 of 10000\n",
      "episode 3000 of 10000\n",
      "episode 3500 of 10000\n",
      "episode 4000 of 10000\n",
      "episode 4500 of 10000\n",
      "episode 5000 of 10000\n",
      "episode 5500 of 10000\n",
      "episode 6000 of 10000\n",
      "episode 6500 of 10000\n",
      "episode 7000 of 10000\n",
      "episode 7500 of 10000\n",
      "episode 8000 of 10000\n",
      "episode 8500 of 10000\n",
      "episode 9000 of 10000\n",
      "episode 9500 of 10000\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "for i_episode in range(num_episodes):\n",
    "    if i_episode % 500 == 0:\n",
    "        print(f\"episode {i_episode} of {num_episodes}\")\n",
    "    # Initialize the environment and state\n",
    "    state = create_board()\n",
    "    while state is not None:\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        raw_action = action.item()\n",
    "        assert(is_valid_move(state, raw_action))\n",
    "        next_state = make_move(state, raw_action)\n",
    "        # We need to negate the value from get_score because\n",
    "        # make_move has advanced to the next move in the game,\n",
    "        # which means get_score is telling us the score for\n",
    "        # \"them\" rather than \"us\".\n",
    "        reward = -get_score(next_state)\n",
    "        reward = torch.tensor([reward])\n",
    "        done = is_complete(next_state)\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization\n",
    "        optimize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba9acbf-3c62-42aa-b997-d21f4fa12a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_action(state):\n",
    "    actions = model(torch.stack([state]))\n",
    "    mask = get_invalid_move_mask(state)\n",
    "    actions.masked_fill_(mask, float('-inf'))\n",
    "    return actions.max(1)[1].item()\n",
    "\n",
    "def human_move(x, y):\n",
    "    global state\n",
    "    human_move_index = 3 * y + x\n",
    "    if not is_valid_move(state, human_move_index):\n",
    "        print(\"*** Invalid human move ***\")\n",
    "        return\n",
    "    state = make_move(state, human_move_index)\n",
    "    if is_complete(state):\n",
    "        score = get_score(state)\n",
    "        if score == -1:\n",
    "            print(\"*** Human wins ***\")\n",
    "        else:\n",
    "            print(\"*** Draw ***\")\n",
    "        return\n",
    "    computer_move_index = computer_action(state)\n",
    "    if not is_valid_move(state, computer_move_index):\n",
    "        print(f\"*** Invalid computer move: {computer_move_index} ***\")\n",
    "        return\n",
    "    state = make_move(state, computer_move_index)\n",
    "    print(state)\n",
    "    if is_complete(state):\n",
    "        score = get_score(state)\n",
    "        if score == -1:\n",
    "            print(\"*** Computer wins ***\")\n",
    "        else:\n",
    "            print(\"*** Draw ***\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "382724c6-ef7f-422c-b474-bcc4c6afc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_board(state):\n",
    "    actions = model(torch.stack([state]))\n",
    "    mask = get_invalid_move_mask(state)\n",
    "    actions.masked_fill_(mask, float('-inf'))\n",
    "    return actions.view(3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5209bf0-09f7-4d17-8601-49cf09fb4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1427704-3d69-46b0-b5a2-77c9255e89a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fee5c545-37d7-4b8b-a4f0-5cb221ae82ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1269e+11, -1.4037e+06,  3.1266e+11],\n",
       "        [ 3.1267e+11,  3.1266e+11,  3.1135e+11],\n",
       "        [ 3.1253e+11,  3.1265e+11,  2.9270e+11]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_board(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1701a08-81f4-4f56-8add-57ded88766da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_and_TF",
   "language": "python",
   "name": "conda-env-azureml_py38_PT_and_TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
